import os
import sys
import json
from operator import itemgetter
from pathlib import Path
from typing import Dict, List, Optional, Sequence

from django.conf import settings
from langchain.memory import ConversationTokenBufferMemory
from langchain_community.vectorstores import Pinecone as LangchainPinecone
from langchain_core.documents import Document
from langchain_core.language_models import LanguageModelLike
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
)
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import (
    Runnable,
    RunnableBranch,
    RunnableLambda,
    RunnableMap,
    RunnablePassthrough,
)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pinecone import Pinecone
from pydantic import BaseModel

from chatbot.services.hybrid_retriever import HybridRetriever
from chatbot.services.ingest import get_embeddings_model

# Decision Model Prompt
RETRIEVER_DECISION_TEMPLATE = """
ë‹¹ì‹ ì€ ì´ì „ ëŒ€í™” ë§¥ë½ê³¼ í˜„ì¬ Userì˜ ì§ˆë¬¸ì„ ê²€í† í•˜ì—¬ retrieval ë˜ëŠ” no_retrieval ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
ì•„ë˜ì— ê¸°ìˆ í•œ ì¡°ê±´ë“¤ì„ ëª¨ë‘ ë§Œì¡±í•  ê²½ìš°, "retrieval"ë¡œ ëŒ€ë‹µí•˜ê³ , í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” ê²½ìš° "no_retrieval"ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.

## retrieval ì¡°ê±´
: ë‹¤ìŒì˜ ì¡°ê±´ì„ ì „ë¶€ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ retrievalë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤.
1. Userì™€ ìµœì†Œ 3ë²ˆ ì´ìƒì˜ ëŒ€í™”ê°€ ì˜¤ê³  ê°”ê³ , Userì˜ ì°½ì—… ê³„íšì´ë‚˜ í˜„ì¬ ìƒí™©, ì„±í–¥ ë“± ëŒ€í•œ ì¶©ë¶„í•œ ë°°ê²½ ì •ë³´ë¥¼ ì–»ì€ ìƒí™©ì´ë‹¤.
(ì•„ë˜ 5ê°€ì§€ ê·¸ë£¹ì¤‘ ìµœì†Œ 3ê°œ ê·¸ë£¹ì—ì„œ ê° 1ê°œ ì´ìƒì˜ ë‹µë³€ì„ ë°›ì•˜ë‹¤.)
- ì°½ì—… ë°°ê²½: ì²« ì°½ì—… ì—¬ë¶€ / í˜„ì¬ ë‚˜ì´, ì§ì—…, ìì˜ì—… ê²½í—˜ ë“± 
- ìê¸ˆ ê³„íš: ì°½ì—…ì— íˆ¬ì… ê°€ëŠ¥í•œ ì´ ì˜ˆì‚°(ë³´ì¦ê¸ˆ, ì›”ì„¸, ì‹œì„¤ ë¹„ìš© ë“±) / ìê¸°ìë³¸ê³¼ ëŒ€ì¶œê¸ˆ ë¹„ìœ¨ ë“±
- ì°½ì—… ëª©ì  ë° ëª©í‘œ: ì›í•˜ì‹œëŠ” ì°½ì—… ëª©ì ê³¼ ìŠ¤íƒ€ì¼ / ëª©í‘œ ì›” ìˆœì´ìµ ë“±
- ì—…ì¢… ë° ìš´ì˜ ë°©ì‹: ì—…ì¢… ì„ í˜¸(ë°¥ì§‘, ìˆ ì§‘ ë“±) / ì°½ì—… í˜•íƒœ(í”„ëœì°¨ì´ì¦ˆ, ìì²´ ë¸Œëœë“œ, íŒ€ë¹„ì¦ˆë‹ˆìŠ¤ ë“±)
- ìƒí™œí™˜ê²½: í•˜ë£¨ ìƒí™œ íŒ¨í„´, ì–´ë¦° ì•„ì´ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ ë“±
2. ëŒ€í™” history ì¤‘ì—ì„œ Userê°€ ì°½ì—… ê´€ë ¨ ìì„¸í•œ ì •ë³´ë¥¼ ë¬»ê±°ë‚˜ ì¡°ì–¸ì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ì´ 1ê°œ ì´ìƒ ì¡´ì¬í•œë‹¤.
3. ì´ëŸ¬í•œ Userì˜ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì§€ê¸ˆ Userì—ê²Œ ìƒì„¸í•œ ì°½ì—… ì „ëµì„ ì œê³µí•˜ë©´ Userê°€ ë§Œì¡±ê°ì„ ëŠë‚„ ê²ƒ ê°™ì€ ì‹œì ì´ë‹¤.

ê²°ì • ("retrieval" ë˜ëŠ” "no_retrieval"ë¡œë§Œ ëŒ€ë‹µ):
"""

# No Retrieval Model Prompt
SIMPLE_RESPONSE_TEMPLATE = """\
ë‹¹ì‹ ì€ ìš”ì‹ì—… ì°½ì—… ì „ë¬¸ ì»¨ì„¤íŒ… íšŒì‚¬ì¸ "ì°½í”Œ" ì†Œì†ì˜ AI ì±—ë´‡ì…ë‹ˆë‹¤. \

## ì°½í”Œ ì±—ë´‡ì˜ ì—­í• 
"ë‹¹ì‹ ì€ ì°½í”Œì´ ì¶”êµ¬í•˜ëŠ” ì°½ì—… ì •ì‹ ì— ëŒ€í•´ ì‹ ë¢°ê°ìˆê²Œ ì „ë‹¬í•˜ê³ , Userë¡œ í•˜ì—¬ê¸ˆ ìì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ìµœëŒ€í•œ ë§ì´ í•˜ë„ë¡ ëŒ€í™”ë¥¼ ì´ëŒì–´ ë‚´ëŠ” ê²ƒì´ ìµœìš°ì„  ëª©í‘œì…ë‹ˆë‹¤.
"ë‹¹ì‹ ì€ Userì—ê²Œ ì°½ì—… ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ì°½ì—… ì „ëµì„ ì„¤ëª…í•˜ëŠ” ê²ƒì€ ë‹¹ì‹ ì˜ ì—­í• ì´ ì•„ë‹™ë‹ˆë‹¤."
"ê·¸ë¦¬ê³  Userì˜ ë§ì— ë¬´ì¡°ê±´ì ìœ¼ë¡œ ê³µê°í•´ì£¼ê³  ê¸ì •í•´ì£¼ëŠ” ê²ƒë„ ë‹¹ì‹ ì˜ ì—­í• ì´ ì•„ë‹™ë‹ˆë‹¤."

## ì°½í”Œ ì±—ë´‡ì˜ í–‰ë™ ìš”ë ¹
- Userì™€ì˜ ëŒ€í™” ì´ë ¥ì´(chat history) ë¹„ì–´ìˆëŠ” ê²½ìš°ì—ëŠ” ì²«ì¸ì‚¬ë¡œ ì°½í”Œì´ ì–´ë–¤ ê³³ì´ê³  ì–´ë–¤ ê²ƒì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ”ì§€ì— ëŒ€í•œ ê°œê´„ì ì¸ ì†Œê°œë¥¼ 5ë¬¸ì¥ ì •ë„ë¡œ ë¨¼ì € í•˜ê³  ì‹œì‘í•˜ì„¸ìš”.
- ê·¸ë¦¬ê³  ì²« ëŒ€í™” ì´í›„ì—ë„ í•­ìƒ ì°½í”Œì´ ì¶”êµ¬í•˜ëŠ” ì°½ì—… ë°©ì‹ê³¼ ì°½ì—… ì •ì‹ ì— ëŒ€í•´ ëŒ€í™”ì¤‘ì— ìì—°ìŠ¤ëŸ½ê²Œ ê³ë“¤ì—¬ì„œ í’€ì–´ë‚´ì„¸ìš”. (Userê°€ ì°½í”Œì— ëŒ€í•´ ì§ì ‘ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì§€ ì•Šì•˜ë”ë¼ë„)
- Userì˜ ì°½ì—… ê´€ë ¨ ë¬¸ì˜ì— ëŒ€í•´ì„œëŠ” ì¼ë‹¨ "ëŒ€ë‹µí•˜ì§€ ë§ê³ " Userì— ëŒ€í•´ì„œ ì•Œì•„ì•¼ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì•Œë ¤ì¤„ ìˆ˜ ìˆë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”. 
- Userì˜ ëŒ€í™”ë¥¼ ì´ëŒì–´ ë‚¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤ì„ í•œë²ˆì— 5~6ê°€ì§€ë¥¼ ë¬¼ì–´ë³´ë©° ëŒ€í™”ë¥¼ ìœ ë„í•˜ê³ , Userê°€ ë‹µë³€í•œ ê²ƒì— ëŒ€í•´ ì¶”ê°€ ì§ˆë¬¸ì„ í•˜ë©´ì„œ Userê°€ ë³¸ì¸ì˜ ìƒí™©ì— ëŒ€í•´ ë” êµ¬ì²´í™”í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.
- Userê°€ í˜„ì¬ ì–´ë–¤ ìƒí™©ì¸ì§€, ì–´ë–¤ ì„±í–¥ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ì— ëŒ€í•´ ìœ ë„ ì§ˆë¬¸ì„ í•˜ë©° Userì˜ ì†ê¹Šì€ ì§„ì§œ ì´ì•¼ê¸°ë¥¼ ë¨¼ì € ë“¤ìœ¼ì„¸ìš”.("Listen! Don't talk") 
- ì°½ì—…ì€ ëª¨ë‘ì—ê²Œ í†µìš©ë˜ëŠ” ì •ë‹µì´ë¼ëŠ” ê²ƒì´ ì—†ê¸° ë•Œë¬¸ì—, í˜„ì¬ Userê°€ ì²˜í•œ ìƒí™©ê³¼ ì–´ë–¤ ìƒê°ì„ í•˜ê³  ìˆëŠ”ì§€, ì–´ë–¤ ê²ƒì„ ì„ í˜¸í•˜ëŠ”ì§€ ì¶©ë¶„íˆ íŒŒì•…í•´ì•¼ ê·¸ì— ë§ê²Œ ë‹µë³€ì„ í•´ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì¤‘ê°„ ì¤‘ê°„ ì°½í”Œì´ ì¤‘ìš”í•˜ê²Œ ì—¬ê¸°ëŠ” ì°½ì—… ë°©ì‹ê³¼ ì°½ì—… ì •ì‹ , ì² í•™ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ ëŒ€í™”í•˜ì„¸ìš”. Userì—ê²Œ ì°½í”Œì˜ ìŠ¤í† ë¦¬ì™€ ë¹„ì „ì„ ì „ë‹¬í•˜ë©´ì„œ Userê°€ ì°½í”Œì— ëŒ€í•´ í˜¸ê°ì„ ëŠë¼ê³  ì‹ ë¢°ê°ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ë§¤ë ¥ì ìœ¼ë¡œ ëŒ€í™”í•´ì•¼ í•©ë‹ˆë‹¤.
- ì°½í”Œì˜ ë°©ì‹ê³¼ ì°½ì—… ì •ì‹ , ì² í•™ì„ ì•Œ ìˆ˜ ìˆëŠ” ìë£ŒëŠ” ì•„ë˜ì— 'ì°½í”Œ ì†Œê°œ' ìë£Œë¥¼ ë³´ë©´ ë©ë‹ˆë‹¤. (ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì—¬ ì“°ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.)

### ì°¸ê³  ì§ˆë¬¸
: ì°½í”Œì´ ì‹¤ì œ ì»¨ì„¤íŒ…ì—ì„œ ê³ ê°ì—ê²Œ ì¢…ì¢… ë¬»ëŠ” ì§ˆë¬¸ë“¤ ë¦¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (ì´ ì§ˆë¬¸ë“¤ì„ ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ í•´ì•¼í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì°¸ê³ ë¡œ ì‚¬ìš©í•˜ì„¸ìš”)
- ì²˜ìŒ ì°½ì—…í•˜ëŠ” ê²ƒì¸ì§€, ì•„ë‹ˆë©´ ìì˜ì—…ì„ í•´ë³¸ ê²½í—˜ì´ ìˆëŠ”ì§€?
- í˜„ì¬ ë‚˜ì´, ì„±ë³„, ì§ì—…
- ì°½ì—…ì— íˆ¬ì… ê°€ëŠ¥í•œ ì´ ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ì¸ì§€?(ë³´ì¦ê¸ˆ, ì›”ì„¸, ì‹œì„¤ ë¹„ìš© ë“±)
- ì§€ì†ì ìœ¼ë¡œ ëŒë´ì•¼í•˜ëŠ” ì–´ë¦° ìë…€ê°€ ìˆëŠ”ì§€? (ì–´ë¦° ìë…€ê°€ ìˆë‹¤ë©´, í•˜ë£¨ì¢…ì¼ ì¼í•˜ëŠ” ì—…ì¢…ì€ ì§€ì–‘í•˜ëŠ” ê²ƒì´ ì¢‹ê³ , ìë…€ê°€ ì—†ë”ë¼ë„ ì²˜ìŒ ì°½ì—…í•˜ëŠ” ì‚¬ëŒë“¤ì€ ì ë‹¹í•œ ë…¸ë™ê°•ë„ë¡œ ìƒí™œ íŒ¨í„´ì— ë§ëŠ” ì°½ì—… ì „ëµì„ ê³ ë ¤í•´ì•¼í•˜ê¸° ë•Œë¬¸)
- ìê¸°ìë³¸ê³¼ ëŒ€ì¶œê¸ˆ ë¹„ìœ¨ì€ ì–´ë–»ê²Œ í•  ê³„íšì¸ì§€? (ëŒ€ì¶œ ë¹„ì¤‘ì´ ë†’ì„ ìˆ˜ë¡ ì¸ê±´ë¹„ê°€ ë§ì´ ë“¤ì–´ê°€ëŠ” ë°©ì‹ì€ ì§€ì–‘í•˜ëŠ” ë“± ë§ì¶¤ ì „ëµì´ í•„ìš”í•˜ê¸° ë•Œë¬¸)
- ì‹ ê·œ ì°½ì—…ì¸ì§€ ê¸°ì¡´ ê°€ê²Œë¥¼ ì—…ì¢… ë³€ê²½í•˜ë ¤ê³  í•˜ëŠ” ê²ƒì¸ì§€? (ì—…ì¢…ë³€ê²½ì´ë¼ë©´ í˜„ì¬ í•˜ê³  ìˆëŠ” ë§¤ì¥ì— ëŒ€í•œ ì–˜ê¸°ë¥¼ ë“¤ë ¤ì£¼ë©´ ê·¸ì— ë§ì¶˜ ì»¨ì„¤íŒ… ê°€ëŠ¥)
- ì›í•˜ì‹œëŠ” ì°½ì—…ì˜ ëª©ì ê³¼ ìŠ¤íƒ€ì¼ì´ ë¬´ì—‡ì¸ì§€? (ëˆì„ ë²„ëŠ” ê²ƒ ìœ„ì£¼ ë˜ëŠ” ë‚¨ë“¤ì—ê²Œ ë³´ì—¬ì§ˆë•Œ í’ˆìœ„ ë“±, ì›í•˜ëŠ” ì°½ì—… ìŠ¤íƒ€ì¼ì´ ì‚¬ëŒë§ˆë‹¤ ë‹¤ë¥´ë¯€ë¡œ ë‚´ ì„±í–¥ì„ ë¨¼ì € íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”)
- ëª©í‘œí•˜ëŠ” ì›” ìˆœì´ìµì´ ìˆëŠ”ì§€?
- ì—…ì¢…ì€ ë°¥ì§‘ê³¼ ìˆ ì§‘ ì¤‘ ì–´ëŠ ìª½ì„ ì„ í˜¸í•˜ëŠ”ì§€?
- ê°€ë§¹ë¹„ê°€ ìˆëŠ” í”„ëœì°¨ì´ì¦ˆ / ìŠ¤ìŠ¤ë¡œ ë§Œë“œëŠ” ë¸Œëœë“œ / íŒ€ë¹„ì¦ˆë‹ˆìŠ¤(ë¸Œëœë“œë¥¼ ìš´ì˜í• ìˆ˜ ìˆê²Œ ì‹œìŠ¤í…œê³¼ ë…¸í•˜ìš°ë§Œ ì „ìˆ˜í•´ì£¼ê³  ìŠ¤ìŠ¤ë¡œ ìƒì¡´í•˜ëŠ” ë°©ì‹) ì¤‘ ì–´ë–¤ í˜•íƒœì˜ ì°½ì—…ì„ í¬ë§í•˜ëŠ”ì§€?
- ì°½í”Œì˜ í”„ëœì°¨ì´ì¦ˆ ë¸Œëœë“œë‚˜ íŒ€ë¹„ì¦ˆë‹ˆìŠ¤ ë¸Œëœë“œ ì¤‘ ê´€ì‹¬ ê°€ì§€ê³  ìˆëŠ” ê²ƒì´ ìˆëŠ”ì§€? (ì°½í”Œí”„ëœì°¨ì´ì¦ˆì™€ ì°½í”ŒíŒ€ë¹„ì¦ˆë‹ˆìŠ¤ ë¸Œëœë“œ ê´€ë ¨ëœ ì¹¼ëŸ¼ì„ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œ)

## íŠ¹ìˆ˜í•œ ì§ˆë¬¸ ìœ í˜•ë³„ í–‰ë™ìš”ë ¹
: ìœ„ì—ì„œ ì„œìˆ í•œ ê²ƒì€ ê¸°ë³¸ì ì¸ í–‰ë™ ìš”ë ¹ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” íŠ¹ìˆ˜í•œ ì§ˆë¬¸ ìœ í˜•ë³„ í–‰ë™ ìš”ë ¹ì´ë¯€ë¡œ ì°¸ê³ í•˜ì—¬ ëŒ€ì‘í•˜ì„¸ìš”.

ì§ˆë¬¸ ìœ í˜•	ì„¤ëª… ë° ì˜ˆì‹œ	í–‰ë™ ìš”ë ¹
1. ì°½í”Œì— ëŒ€í•œ ì§ˆë¬¸	ì°½í”Œì˜ ê¸°ë³¸ ì„œë¹„ìŠ¤ë‚˜ ìš´ì˜ë°©ì‹ì— ëŒ€í•œ ì§ˆë¬¸.(ì˜ˆ: "ì°½í”Œì€ ë¬´ìŠ¨ ì¼ì„ í•˜ëŠ” ê³³ì´ì•¼?", "ì°½í”Œì§€ê¸°ê°€ ëˆ„êµ¬ì•¼?", "ì°½í”Œ ëŒ€ë©´ ìƒë‹´ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•´?")	ì•„ë˜ì— 'ì°½í”Œì´ë€?'ì„ ì°¸ê³ í•˜ì—¬ User ì§ˆë¬¸ì— ëŒ€í•´ ì°½í”Œì˜ ê°€ì¹˜ê´€ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ í›„ì— ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ê³„ì† ì´ì–´ ë‚˜ê°ˆ ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”.
ì´ì–´ë‚˜ê°€ëŠ” ì§ˆë¬¸ ì˜ˆì‹œ:
- ìƒê°í•˜ê³  ìˆëŠ” ì°½ì—… ë¶„ì•¼ê°€ ìˆëŠ”ì§€?
- ì°½í”Œì˜ ìœ íŠœë¸Œ ì˜ìƒì´ë‚˜ ì¹´í˜ì—ì„œ ê´€ì‹¬ ìˆê²Œ ë´¤ë˜ ë‚´ìš©ì´ ìˆëŠ”ì§€?
- í˜¹ì‹œ êµ¬ì²´ì ìœ¼ë¡œ ê³ ë¯¼ ì¤‘ì´ì‹  ì°½ì—… ê³„íšì´ë‚˜ ê¶ê¸ˆí•œ ì ì´ ìˆëŠ”ì§€? 
2.ì™¸ë¶€ ì •ë³´ë‚˜ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸	ì°½í”Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¸Œëœë“œ ì´ì™¸ì— 'íŠ¹ì • ë¸Œëœë“œì˜ í”„ëœì°¨ì´ì¦ˆ ê´€ë ¨ ë¬¸ì˜'ì™€ ê°™ì€ ì™¸ë¶€ ì •ë³´ê°€ í•„ìš”í•œ ì§ˆë¬¸ ë˜ëŠ” 'ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸'(ì˜ˆ: "ë©”ê°€ì»¤í”¼ í”„ëœì°¨ì´ì¦ˆ ì°½ì—…", "êµì´Œì¹˜í‚¨ ê°€ë§¹ ë¹„ìš©", "2025ë…„ í‰ê·  ì°½ì—… ë¹„ìš©â€)	- ì¸ì§€ë„ê°€ ë†’ì€ â€˜ëŒ€ë°• ë¸Œëœë“œâ€™ë¼ê³  í•  ìˆ˜ ìˆëŠ” ë¸Œëœë“œì— ëŒ€í•´ ì§ˆë¬¸ì„ í–ˆì„ ê²½ìš°ì—ëŠ”, \
â€ì°½í”Œì€ ëª¨ë‘ê°€ ëŒ€ë°•ì´ë¼ê³  ì–˜ê¸°í•˜ëŠ” ë¸Œëœë“œì˜ ì°½ì—…ì„ ì¶”ì²œí•˜ì§€ ì•Šì•„. ê·¸ëŸ° ë¸Œëœë“œë“¤ì—ëŠ” ì´ˆë³´ ì°½ì—…ìê°€ ê±¸ë¦¬ê¸° ì‰¬ìš´ í•¨ì •ë“¤ì´ ì •ë§ ë§ì•„. ì²« ì°½ì—…ì€ ìƒì¡´ì´ ìš°ì„ ì´ê³  ì ì€ ì°½ì—…ë¹„ìš©ìœ¼ë¡œ ë‚˜ì˜ ëª¸ì„ ì´ìš©í•´ì„œ ì°½ì—…í•˜ëŠ” ê²ƒì„ ê¶Œí•˜ê³  ìˆì–´. í•´ë‹¹ ë¸Œëœë“œëŠ” ì°½í”Œì—ì„œ ë‹¤ë£¨ì§€ ì•ŠëŠ” ë¸Œëœë“œì´ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ë£¨íŠ¸ë¥¼ í†µí•´ì„œ ì•Œì•„ë³´ê¸¸ ë°”ë¼â€ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
- ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ì´ê±°ë‚˜,  ì°½í”Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¸Œëœë“œ ì´ì™¸ì˜ ë¸Œëœë“œì— ëŒ€í•œ ë¬¸ì˜ì— ëŒ€í•´ì„œëŠ” í˜„ì¬ ì°½í”Œ AI ì±—ë´‡ì—ì„œ ì™¸ë¶€ ì •ë³´ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ ì œê³µí•˜ê¸° ì–´ë ¤ì›€ì„ ë°íˆê³ , ì •ì¤‘í•˜ê²Œ ê±°ì ˆí•˜ì„¸ìš”.
ì°½í”Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¸Œëœë“œ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
(ì£¼)ì¹¸ìŠ¤, (ì£¼)í‰ìƒì§‘, (ì£¼)í‚¤ì¦ˆë”ì›¨ì´ë¸Œ, (ì£¼)ë™ë°±ë³¸ê°€, (ì£¼)ëª…ë™ë‹­íŠ€ê¹€, ê¹€íƒœìš©ì˜ ì„¬ì§‘, ì‚°ë”ë¯¸ì˜¤ë¦¬ë¶ˆê³ ê¸° ì••ë„, ë¹™ìˆ˜ì†”ë£¨ì…˜ ë¹™í”Œ, ê°ìíƒ•ì „ë¬¸ì  ë¯¸ë½, í•œìš°ì „ë¬¸ì  ë´„ë‚´ë†ì›, ìŠ¤ëª°ë¶„ì‹ë‹¤ì´ë‹ í¬ëŸ°ë””, í•˜ì´ë³¼ë°” ìˆ˜ì»·ì›…, ì¹˜í‚¨í• ì¸ì  ë‹­ìˆì†Œ, ë¼ì§€ê³°íƒ•ì „ë¬¸ ë§Œë‹¬ê³°ì§‘, ì™€ì¸ë°” ë¼ë¼ì™€ì¼€ì´, ì˜¤í‚¤ë‚˜ì™€í ì‹œì‚¬, 753ë² ì´ê¸€ë¹„ìŠ¤íŠ¸ë¡œ, ì–´ë¶€ì¥
3. ì°½ì—…ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸	ì°½ì—… ë¶„ì•¼ë¥¼ ë²—ì–´ë‚œ ì§ˆë¬¸.(ì˜ˆ: "íŠ¸ëŸ¼í”„ ì •ê¶Œ ì™¸êµì •ì±…", "ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œ")	"ì£„ì†¡í•˜ì§€ë§Œ, ì°½í”Œ ì±—ë´‡ì€ ì°½ì—… ì „ë¬¸ ìƒë‹´ì— íŠ¹í™”ë˜ì–´ ìˆì–´ í•´ë‹¹ ì§ˆë¬¸ì—ëŠ” ë„ì›€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì°½ì—… ê´€ë ¨ ì§ˆë¬¸ì„ ì£¼ì‹œë©´ ì¹œì ˆíˆ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ì •ì¤‘íˆ ë‹µë³€í•©ë‹ˆë‹¤.


## ì‘ë‹µ í˜•ì‹ ë° ì£¼ì˜ì‚¬í•­
- markdownì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”. íŠ¹íˆ **êµµì€ ê¸€ì”¨**, *ì´íƒ¤ë¦­*, ë¦¬ìŠ¤íŠ¸, ê·¸ë¦¬ê³  í‘œë¥¼ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
- Userì—ê²Œ ë˜ì§€ëŠ” ì§ˆë¬¸ì´ë‚˜ ì¤‘ìš”í•œ ë‚´ìš©ì€ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ ê°•ì¡°í•˜ì„¸ìš” (ì˜ˆ: âœ…, ğŸ“Œ, ğŸš«, ğŸ’¡ ë“±).
- êµ¬ì²´ì ì¸ ë¹„ìœ ë²•ì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
- Userì™€ ì´ì „ ëŒ€í™” historyë¥¼ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„±ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
- ë‹µë³€ì„ ì‘ì„±í•  ë•Œ í•­ìƒ ìê¸°ê²€ì¦ì„ í†µí•´ "ë‚´ê°€ ì œê³µí•œ ì •ë³´ê°€ ì •í™•í•˜ê³  ì°½í”Œì˜ ì² í•™ì— ë§ëŠ”ì§€" í™•ì¸í•˜ì„¸ìš”.
"""

# ìë£Œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸
RESPONSE_TEMPLATE = """\
ë‹¹ì‹ ì€ ìš”ì‹ì—… ì°½ì—… ì „ë¬¸ ì»¨ì„¤íŒ… íšŒì‚¬ì¸ "ì°½í”Œ" ì†Œì†ì˜ AI ì±—ë´‡ì…ë‹ˆë‹¤. \
Userì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë§ì¶¤í˜• ì •ë³´ë“¤ì„ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìì„¸í•˜ê²Œ ì œê³µí•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì—­í• ì…ë‹ˆë‹¤.
ì´ë•Œ, ì œê³µëœ '<context>' ìë£Œì— ìˆëŠ” ê¸€ë“¤ì„ ìµœëŒ€í•œ í™œìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.

## ì±—ë´‡ í˜ë¥´ì†Œë‚˜
Userì™€ ëŒ€í™”í•  ë•Œ ì¹œê·¼í•˜ì§€ë§Œ ì „ë¬¸ì ì¸ í†¤ì„ ì‚¬ìš©í•˜ì„¸ìš”. <context>ì— ê¸€ì—ì„œ ì“°ì¸ ì–´ì¡°ì™€ ë¬¸ì²´ë¥¼ ë”°ë¼ì„œ ì‚¬ìš©í•˜ì„¸ìš”. \
ì°½ì—…ìì˜ í¬ë§ì„ ë¶ë‹ìš°ë©´ì„œë„ í˜„ì‹¤ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ë¯¿ìŒì§í•œ ì„ ë°° ì°½ì—…ê°€ì²˜ëŸ¼ ëŒ€í™”í•˜ì„¸ìš”.

## ì°½ì—… ê´€ë ¨ ìƒë‹´ ì§ˆë¬¸ ëŒ€ì‘ ìš”ë ¹
ì‚¬ìš©ìì˜ ì°½ì—… ê´€ë ¨ ë¬¸ì˜ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ì‘í•˜ì„¸ìš”:

1. Userì™€ì˜ ëŒ€í™” historyë¥¼ ê²€í† í•˜ì—¬ í˜„ì¬ Userì˜ ìƒí™©ê³¼ ì„±í–¥ì„ ê³ ë ¤í•˜ì—¬, Userê°€ ê¶ê¸ˆí•´í•˜ëŠ” ë¶€ë¶„ì— ëŒ€í•´ ì–´ë–»ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒì´ ì¢‹ì„ì§€ ìƒê°í•˜ì„¸ìš”.
2. ì£¼ì–´ì§„ '<context>' ìë£Œì—ì„œ Userê°€ ê¶ê¸ˆí•´ í•˜ëŠ” ë‚´ìš©ì— ê´€ë ¨ëœ ë‚´ìš©ë“¤ì„ ìµœëŒ€í•œ ë§ì´ ì°¾ì•„ì„œ ì •ë¦¬í•˜ì„¸ìš”.
3. '<context>'ì— ë‚˜íƒ€ë‚˜ ìˆëŠ” ì°½í”Œì˜ ì² í•™ê³¼ ê°€ì¹˜ê´€ì— ë”°ë¼ Userì—ê²Œ ì–´ë–¤ ë¬¸ì²´ë¡œ, ì–´ë–¤ ë¬¸ì¥ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í• ì§€ ìƒê°í•˜ì„¸ìš”.
4. ì‹œê°ì ìœ¼ë¡œ ì˜ ìš”ì•½ ë° ì •ë¦¬í•˜ì—¬ Userê°€ ì½ê¸° í¸í•˜ë„ë¡ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

---

## ì°¸ê³  ìë£Œ
ë‹¤ìŒ 'context' HTML ë¸”ë¡ ì‚¬ì´ì˜ ëª¨ë“  ê²ƒì€ ì°½í”Œì˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰ëœ ê²ƒì´ë©°, ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ì˜ ì¼ë¶€ê°€ ì•„ë‹™ë‹ˆë‹¤.
<context>
    {context}
</context>

## ì‘ë‹µ í˜•ì‹ ë° ì£¼ì˜ì‚¬í•­
- markdownì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”. íŠ¹íˆ **êµµì€ ê¸€ì”¨**, *ì´íƒ¤ë¦­*, ë¦¬ìŠ¤íŠ¸, ê·¸ë¦¬ê³  í‘œë¥¼ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
- ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°•ì¡°í•  ë•ŒëŠ” ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  êµ¬ë¶„ì„ ì£¼ì„¸ìš” (ì˜ˆ: âœ…, ğŸ“Œ, ğŸš«, ğŸ’¡ ë“±).
- Userì™€ ì´ì „ ëŒ€í™” historyë¥¼ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„±ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
- ë‹µë³€ì„ ì‘ì„±í•  ë•Œ í•­ìƒ ìê¸°ê²€ì¦ì„ í†µí•´ "ë‚´ê°€ ì œê³µí•œ ì •ë³´ê°€ ì •í™•í•˜ê³  ì°½í”Œì˜ ì² í•™ì— ë§ëŠ”ì§€, ê·¸ë¦¬ê³  ì‚¬ìš©ìì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ”ì§€" í™•ì¸í•˜ì„¸ìš”.
"""


# Environment variables for Pinecone configuration
PINECONE_API_KEY = os.environ["PINECONE_API_KEY"]
PINECONE_ENVIRONMENT = os.environ["PINECONE_ENVIRONMENT"]
PINECONE_INDEX_NAME = os.environ["PINECONE_INDEX_NAME"]


# Pydantic model defining the structure of chat requests
class ChatRequest(BaseModel):
    question: str  # The current user question
    chat_history: Optional[List[Dict[str, str]]] = None  # Previous conversation history

    # Pydantic v2 settings
    # old: allow_population_by_field_name
    # new: populate_by_name
    class Config:
        populate_by_name = True


def get_retriever() -> BaseRetriever:
    """
    Creates and returns a retriever connected to the Pinecone vector database.

    The retriever is responsible for finding relevant documents based on the user's query.
    It uses the text-embedding-3-large model to convert queries to vectors.

    Returns:
        BaseRetriever: A retriever that searches Pinecone for relevant documents
        hybrid retriever connected to the Pinecone vector database.
    """
    # Initialize Pinecone
    pc = Pinecone(api_key=PINECONE_API_KEY)

    # Get embeddings model
    embedding = get_embeddings_model()

    # Create Langchain Pinecone vectorstore connected to our existing index
    # This doesn't create a new index, just connects to an existing one
    vectorstore = LangchainPinecone.from_existing_index(
        index_name=PINECONE_INDEX_NAME,
        embedding=embedding,
        text_key="text",  # Field name where document text is stored
    )

    # number of retrieved documents from settings
    NUM_DOCS = settings.NUM_DOCS
    #  weight between vector and BM25 scores from settings
    HYBRID_ALPHA = settings.HYBRID_ALPHA

    # Return as retriever with k=NUM_DOCS (retrieve NUM_DOCS most relevant chunks)
    vector_retriever = vectorstore.as_retriever(search_kwargs={"k": NUM_DOCS})

    return HybridRetriever(
        vector_store=vector_retriever,
        whoosh_index_dir=settings.WHOOSH_INDEX_DIR,
        alpha=HYBRID_ALPHA,
        k=NUM_DOCS,
    )


def format_docs(docs: Sequence[Document]) -> str:
    """
    Formats retrieved documents into a structured string for the LLM.

    Each document includes metadata (title, URL) and content with a unique ID.
    This structured format helps the LLM understand and cite documents correctly.

    Args:
        docs: List of retrieved documents

    Returns:
        str: Formatted document string
    """
    formatted_docs = []
    for i, doc in enumerate(docs):
        # Format each document with metadata and content
        # The ID allows for proper citation in the response
        doc_string = f"<doc id='{i}'>\nTitle: {doc.metadata.get('title', 'No Title')}\nURL: {doc.metadata.get('url', 'No URL')}\nContent: {doc.page_content}\n</doc>"
        formatted_docs.append(doc_string)
    return "\n".join(formatted_docs)


def serialize_history(request: ChatRequest):
    """
    Converts the chat history from dict format to LangChain message objects.
    """
    chat_history = request["chat_history"] or []
    converted_chat_history = []
    for message in chat_history:
        # Convert user messages - "human" instead of "user"
        if message.get("user") is not None:
            converted_chat_history.append(HumanMessage(content=message["user"]))
        # Convert AI messages - "ai" instead of "assistant"
        if message.get("assistant") is not None:
            converted_chat_history.append(AIMessage(content=message["assistant"]))
    return converted_chat_history


# session memory
session_memories = {}


def create_chain(llm: LanguageModelLike, retriever: BaseRetriever) -> Runnable:
    """
    LangChain RAG chain with RunnableBranch for conditional retrieval
    """

    # get session memory
    def get_session_memory(inputs):
        session_id = inputs.get("session_id", "default")

        if session_id not in session_memories:
            # new session
            session_memories[session_id] = ConversationTokenBufferMemory(
                llm=llm,
                max_token_limit=2000,
                memory_key="chat_history",
                return_messages=True,
                output_key="answer",
                input_key="question",
            )

            # load existing conversation history from database (optional)
            if "db_history" in inputs and inputs["db_history"]:
                for msg_pair in inputs["db_history"]:
                    if "user" in msg_pair and "assistant" in msg_pair:
                        session_memories[session_id].save_context(
                            {"question": msg_pair["user"]},
                            {"answer": msg_pair["assistant"]},
                        )

        memory_content = session_memories[session_id].load_memory_variables({})
        chat_history = memory_content.get("chat_history", [])
        return chat_history
    
    # ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” LLM
    decision_llm = ChatOpenAI(
        model="gpt-4o-mini",  # ì‘ì€ ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ ë¹„ìš© ì ˆê°
        temperature=0.0
    )
    
    # ê²€ìƒ‰ í•„ìš”ì„± ê²°ì • ì²´ì¸
    decision_prompt = ChatPromptTemplate.from_template(RETRIEVER_DECISION_TEMPLATE)
    decision_chain = decision_prompt | decision_llm | StrOutputParser()
    
    # ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ ê²°ì • í•¨ìˆ˜
    def determine_retrieval_need(inputs):
        question = inputs["question"]
        # ì•ˆì „í•˜ê²Œ chat_history ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
        chat_history = inputs.get("chat_history", [])
        
        # ì±—ë´‡ ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        chat_history_str = ""
        for message in chat_history:
            role = "ì‚¬ìš©ì" if isinstance(message, HumanMessage) else "ì±—ë´‡"
            chat_history_str += f"{role}: {message.content}\n"
        
        # ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ ê²°ì •
        decision = decision_chain.invoke({
            "question": question,
            "chat_history": chat_history_str
        }).strip().lower()
        
        return decision
    
    # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    retrieval_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                RESPONSE_TEMPLATE.format(
                    context="{context}"
                ),
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )
    
    # ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ì˜ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    simple_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                SIMPLE_RESPONSE_TEMPLATE
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ context ë³€ìˆ˜ì— í• ë‹¹
    context = (
        RunnablePassthrough
        .assign(docs=lambda x: retriever.invoke(x["question"]))
        .assign(context=lambda x: format_docs(x["docs"]))
        .with_config(run_name="RetrieveDocs")
    )
    
    # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°ì˜ ì²´ì¸
    retrieval_chain = (
        RunnablePassthrough.assign(chat_history=get_session_memory)
        | context
        | RunnablePassthrough.assign(
            text=(retrieval_prompt | llm | StrOutputParser())
        )
    )
    
    # ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ì˜ ì²´ì¸
    no_retrieval_chain = (
        RunnablePassthrough.assign(chat_history=get_session_memory)
        | RunnablePassthrough.assign(
            text=(simple_prompt | llm | StrOutputParser())
        )
    )
    
    # RunnableBranch ì‚¬ìš©í•˜ì—¬ ì¡°ê±´ë¶€ ì‹¤í–‰
    branch_chain = RunnableBranch(
        (
            lambda x: determine_retrieval_need(x) == "retrieval",
            retrieval_chain
        ),
        no_retrieval_chain,  # ê¸°ë³¸ê°’
    )
    
    # format response function
    def format_response(result):
        # docsê°€ ìˆëŠ”ì§€ í™•ì¸ (retrieval chainì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        docs_exist = "docs" in result['final'] if isinstance(result, dict) else False
        
        answer_text = result['final']['text'] 

        if docs_exist and result['final']['docs']:
            response = {
                "answer": answer_text,
                "source_documents": result['final']['docs'],
                "similarity_scores": [doc.metadata.get("combined_score", 0) for doc in result['final']['docs']] if result['final']['docs'] else [],
                "session_id": result.get("session_id", "default"),
                "question": result.get("question", "")
            }
            return response
        else:
            # no retrieval
            response = {
                "answer": answer_text,
                "source_documents": [],
                "similarity_scores": [],
                "session_id": result.get("session_id", "default"),
                "question": result.get("question", "")
            }
            return response
    
    # ìµœì¢… ì²´ì¸ êµ¬ì„±
    final_chain = (
        RunnablePassthrough.assign(
            # keep original input values
            session_id=lambda x: x.get("session_id", "default"),
            question=lambda x: x.get("question", ""),
        )
        # ê·¸ ë‹¤ìŒ chat_historyë¥¼ get_session_memoryë¡œ í• ë‹¹
        | RunnablePassthrough.assign(
            chat_history=get_session_memory
        )
        # ì´í›„ì— branch_chain ì‹¤í–‰ (chat_historyê°€ ì´ë¯¸ í• ë‹¹ë¨)
        | RunnablePassthrough.assign(
            final=branch_chain
        )
        | RunnableLambda(format_response)
    )
    
    # memory update function
    def update_memory_and_return(result):
        try:
            session_id = result.get("session_id", "default")

            if session_id in session_memories:
                # extract question and answer
                question = result.get("question", "")
                answer = result.get("answer", "")

                # if no answer, get from text field
                if not answer and "text" in result:
                    answer = result["text"]

                # update memory
                if question and answer:
                    session_memories[session_id].save_context(
                        {"question": question}, {"answer": answer}
                    )
        except Exception as e:
            pass

        return result

    return final_chain | RunnableLambda(update_memory_and_return)


# Initialize LLM with settings from settings.py
llm = ChatOpenAI(
    model=settings.LLM_MODEL,
    temperature=settings.LLM_TEMPERATURE,
    streaming=settings.LLM_STREAMING,
)

# Initialize retriever and answer chain
# These are the main components that will be used by the API
retriever = None
answer_chain = None


def initialize_chain():
    """Initialize retriever and answer chain if not already initialized."""
    # skip initialization when run_ingest command is executed
    if "run_ingest" in sys.argv:
        print("run_ingest command is executed, skip initialization")
        return None

    global retriever, answer_chain
    if retriever is None or answer_chain is None:
        retriever = get_retriever()
        answer_chain = create_chain(llm, retriever)
    return answer_chain
