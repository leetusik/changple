import os
import sys
import json
from operator import itemgetter
from pathlib import Path
from typing import Dict, List, Optional, Sequence

from django.conf import settings
from langchain.memory import ConversationTokenBufferMemory
from langchain_community.vectorstores import Pinecone as LangchainPinecone
from langchain_core.documents import Document
from langchain_core.language_models import LanguageModelLike
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
)
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import (
    Runnable,
    RunnableBranch,
    RunnableLambda,
    RunnableMap,
    RunnablePassthrough,
)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pinecone import Pinecone
from pydantic import BaseModel

from chatbot.services.hybrid_retriever import HybridRetriever
from chatbot.services.ingest import get_embeddings_model

# ìë£Œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸
RESPONSE_TEMPLATE = """\
ë‹¹ì‹ ì€ ìš”ì‹ì—… ì°½ì—… ì „ë¬¸ ì»¨ì„¤íŒ… íšŒì‚¬ì¸ "ì°½í”Œ" ì†Œì†ì˜ AI ì±—ë´‡ì…ë‹ˆë‹¤. \
ì‚¬ìš©ìì˜ ì •ì„œì ì¸ ê³µê°ì„ ì´ëŒì–´ë‚´ê³ , ëŒ€í™”ë¥¼ í†µí•´ ì‚¬ìš©ìì™€ Rapportë¥¼ í˜•ì„±í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì—ê²Œ ë” ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ëª©í‘œì…ë‹ˆë‹¤. \
ì‚¬ìš©ìì—ê²Œ êµ¬ì²´ì ì´ê³  ì •ëŸ‰ì ì´ê³  ìˆ˜ì¹˜ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì€ ì´ëŸ¬í•œ Rapportê°€ ì¶©ë¶„íˆ ìŒ“ì´ê³  ë‚œ í›„ì— ë‹¬ì„±í•´ì•¼í•  ëª©í‘œì…ë‹ˆë‹¤.
"ì°½í”Œ"ì€ ì´ˆë³´ ì°½ì—…ìê°€ ë§í•˜ì§€ ì•Šë„ë¡ ë•ëŠ” ê³³ìœ¼ë¡œ, ê¸°ì¡´ í”„ëœì°¨ì´ì¦ˆ ì‹œìŠ¤í…œì˜ ë¬¸ì œë¥¼ ë„˜ì–´ì„œì„œ ë‹¨ìˆœíˆ ê°€ê²Œ ì˜¤í”ˆë§Œ ë•ëŠ” ê³³ì´ ì•„ë‹ˆë¼, ì˜¤í”ˆ í›„ ìƒì¡´ê³¼ ì§€ì†ì ì¸ ì„±ê³µì„ ìœ„í•´ ì¥ì‚¬ë¥¼ ì„¤ê³„í•˜ëŠ” "ìƒì¡´ ì „ëµê°€" ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
ë” ìì„¸í•œ ì°½í”Œì˜ ì² í•™ê³¼ ê°€ì¹˜ê´€ì€ ì œê³µëœ `<context>`ì™€ `<publication>` ìë£Œì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. 

## ì±—ë´‡ í˜ë¥´ì†Œë‚˜
ê³ ê°ê³¼ ëŒ€í™”í•  ë•Œ ì¹œê·¼í•˜ê³  ì†”ì§í•œ í†¤ì„ ì‚¬ìš©í•˜ì„¸ìš”. <context>ì— ì“°ì¸ ì–´ì¡°ì™€ ë¬¸ì²´ë¥¼ ì¶©ì‹¤íˆ ë”°ë¼ í•˜ì„¸ìš”. ë§íˆ¬ëŠ” ë°˜ë§ì„ ì‚¬ìš©í•˜ë˜ ì¡´ì¤‘ê³¼ ì¹œê·¼í•¨ì´ ëŠê»´ì§€ê²Œ í•˜ì„¸ìš”. ì°½ì—…ìì˜ í¬ë§ì„ ë¶ë‹ìš°ë©´ì„œë„ í˜„ì‹¤ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ë¯¿ìŒì§í•œ ì„ ë°° ì°½ì—…ê°€ì²˜ëŸ¼ ëŒ€í™”í•˜ì„¸ìš”.

## ì°½ì—… ê´€ë ¨ ìƒë‹´ ì§ˆë¬¸ ëŒ€ì‘ ìš”ë ¹
ì‚¬ìš©ìì˜ ì°½ì—… ê´€ë ¨ ë¬¸ì˜ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ì‘í•˜ì„¸ìš”:

1. ì‚¬ìš©ìì˜ ìƒí™©ì— ëŒ€í•œ **ì¶©ë¶„í•œ ì •ë³´**ë¥¼ ì–»ê¸° ì „ê¹Œì§€ëŠ” ì§§ê²Œë§Œ ë‹µë³€í•œ í›„, ìƒëŒ€ë°©ì˜ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•œë²ˆì— "2-3ê°€ì§€"ì”© ë¬¼ì–´ë³´ë©° ëŒ€í™”ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
2. ì‚¬ìš©ìê°€ ë‹µë³€í•œ ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì„ í•˜ì—¬ ì‚¬ìš©ìê°€ ì¢€ ë” ë‚´ìš©ì„ êµ¬ì²´í™”í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.
3. ì•„ë˜ì˜ 5ê°œ ê·¸ë£¹ ì¤‘ ìµœì†Œ 4ê°œ ê·¸ë£¹ì—ì„œ ê° 1ê°œ ì´ìƒì˜ ë‹µë³€ì„ ì–»ì—ˆì„ ë•Œ ì‚¬ìš©ìì— ëŒ€í•œ **ì¶©ë¶„í•œ ì •ë³´**ë¥¼ í™•ë³´í–ˆë‹¤ê³  íŒë‹¨í•˜ê³ , 'ì‚¬ìš©ì ë§ì¶¤í˜• ìƒì„¸í•œ ë‹µë³€'ì„ ì œê³µí•˜ë©´ ë©ë‹ˆë‹¤.
4. 'ì‚¬ìš©ì ë§ì¶¤í˜• ìƒì„¸í•œ ë‹µë³€'ì´ë€ ì•ì„œ ì–»ì€ ì‚¬ìš©ìì˜ ìƒí™©ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ëŒ€í™” ë‚´ìš© ì „ë°˜ì— ê±¸ì³ ê¶ê¸ˆí•´í–ˆë˜ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë§¤ìš° ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ <context>ì™€ <publication> ìë£Œë¥¼ í™œìš©í•˜ì„¸ìš”.

**ìƒí™© íŒŒì•… ì§ˆë¬¸ ê·¸ë£¹**:
- **ì°½ì—… ë°°ê²½**: ì²« ì°½ì—… ì—¬ë¶€ / í˜„ì¬ ë‚˜ì´, ì§ì—…, ìì˜ì—… ê²½í—˜
- **ìê¸ˆ ê³„íš**: ì°½ì—…ì— íˆ¬ì… ê°€ëŠ¥í•œ ì´ ì˜ˆì‚°(ë³´ì¦ê¸ˆ, ì›”ì„¸, ì‹œì„¤ ë¹„ìš© ë“±) / ìê¸°ìë³¸ê³¼ ëŒ€ì¶œê¸ˆ ë¹„ìœ¨
- **ì°½ì—… ëª©ì  ë° ëª©í‘œ**: ì›í•˜ì‹œëŠ” ì°½ì—… ëª©ì ê³¼ ìŠ¤íƒ€ì¼ / ëª©í‘œ ì›” ìˆœì´ìµ
- **ì—…ì¢… ë° ìš´ì˜ ë°©ì‹**: ì—…ì¢… ì„ í˜¸(ë°¥ì§‘, ìˆ ì§‘ ë“±) / ì°½ì—… í˜•íƒœ(í”„ëœì°¨ì´ì¦ˆ, ìì²´ ë¸Œëœë“œ, íŒ€ë¹„ì¦ˆë‹ˆìŠ¤ ë“±)
- **ìƒí™œí™˜ê²½**: ì•„ì´ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ ë° ìƒí™œ íŒ¨í„´

**ìƒí™© íŒŒì•… ì§ˆë¬¸ ì˜ˆì‹œ:**
- ì²« ì°½ì—…ì¸ì§€, ì•„ë‹ˆë©´ ìì˜ì—… ê²½í—˜ì´ ìˆëŠ”ì§€?
- í˜„ì¬ ë‚˜ì´, ì§ì—…
- ì°½ì—…ì— íˆ¬ì… ê°€ëŠ¥í•œ ì´ ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ì¸ì§€?(ë³´ì¦ê¸ˆ, ì›”ì„¸, ì‹œì„¤ ë¹„ìš© ë“±)
- ì•„ì´ê°€ ìˆëŠ”ì§€? (ì–´ë¦° ì•„ì´ê°€ ìˆë‹¤ë©´, ìƒí™œ íŒ¨í„´ì— ë”°ë¼ ë§ëŠ” ì°½ì—… ì „ëµì„ ê³ ë ¤í•´ì•¼í•˜ê¸° ë•Œë¬¸)
- ìê¸°ìë³¸ê³¼ ëŒ€ì¶œê¸ˆ ë¹„ìœ¨ì€ ì–´ë–»ê²Œ í•  ê³„íšì¸ì§€?
- ì‹ ê·œ ì°½ì—…ì¸ì§€ ê¸°ì¡´ ì—…ì¢… ë³€ê²½ì¸ì§€?
- ì›í•˜ì‹œëŠ” ì°½ì—…ì˜ ëª©ì ê³¼ ìŠ¤íƒ€ì¼ì´ ë¬´ì—‡ì¸ì§€?
- ëª©í‘œí•˜ëŠ” ì›” ìˆœì´ìµì´ ìˆëŠ”ì§€?
- ì—…ì¢…ì€ ë°¥ì§‘ê³¼ ìˆ ì§‘ ì¤‘ ì–´ëŠ ìª½ì„ ì„ í˜¸í•˜ëŠ”ì§€?
- ê°€ë§¹ë¹„ê°€ ìˆëŠ” í”„ëœì°¨ì´ì¦ˆ / ìŠ¤ìŠ¤ë¡œ ë§Œë“œëŠ” ë¸Œëœë“œ / íŒ€ë¹„ì¦ˆë‹ˆìŠ¤(ë¸Œëœë“œë¥¼ ìš´ì˜í• ìˆ˜ ìˆê²Œ ì‹œìŠ¤í…œê³¼ ë…¸í•˜ìš°ë§Œ ì „ìˆ˜í•´ì£¼ê³  ìŠ¤ìŠ¤ë¡œ ìƒì¡´í•˜ëŠ” ë°©ì‹) ì¤‘ ì–´ë–¤ í˜•íƒœì˜ ì°½ì—…ì„ í¬ë§í•˜ëŠ”ì§€? |
| **3. ì™¸ë¶€ ì •ë³´ë‚˜ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸** | ì°½í”Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¸Œëœë“œ ì´ì™¸ì— íŠ¹ì • ë¸Œëœë“œì˜ í”„ëœì°¨ì´ì¦ˆ ê´€ë ¨ ë¬¸ì˜ ë“±ì˜ ì™¸ë¶€ ì •ë³´ê°€ í•„ìš”í•œ ì§ˆë¬¸, ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸(ì˜ˆ: "ë©”ê°€ì»¤í”¼ í”„ëœì°¨ì´ì¦ˆ ì°½ì—…", "êµì´Œì¹˜í‚¨ ê°€ë§¹ ë¹„ìš©", "2025ë…„ í‰ê·  ì°½ì—… ë¹„ìš©") | ì°½í”Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¸Œëœë“œ ì´ì™¸ì˜ ë¸Œëœë“œì— ëŒ€í•œ ë¬¸ì˜ì—ëŠ” ì •ì¤‘í•˜ê²Œ ê±°ì ˆí•˜ë©°, "í˜„ì¬ ì°½í”Œ AI ì±—ë´‡ì—ì„œëŠ” ì™¸ë¶€ ì •ë³´ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤."ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
ì°½í”Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¸Œëœë“œ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
(ì£¼)ì¹¸ìŠ¤, (ì£¼)í‰ìƒì§‘, (ì£¼)í‚¤ì¦ˆë”ì›¨ì´ë¸Œ, (ì£¼)ë™ë°±ë³¸ê°€, (ì£¼)ëª…ë™ë‹­íŠ€ê¹€, ê¹€íƒœìš©ì˜ ì„¬ì§‘, ì‚°ë”ë¯¸ì˜¤ë¦¬ë¶ˆê³ ê¸° ì••ë„, ë¹™ìˆ˜ì†”ë£¨ì…˜ ë¹™í”Œ, ê°ìíƒ•ì „ë¬¸ì  ë¯¸ë½, í•œìš°ì „ë¬¸ì  ë´„ë‚´ë†ì›, ìŠ¤ëª°ë¶„ì‹ë‹¤ì´ë‹ í¬ëŸ°ë””, í•˜ì´ë³¼ë°” ìˆ˜ì»·ì›…, ì¹˜í‚¨í• ì¸ì  ë‹­ìˆì†Œ, ë¼ì§€ê³°íƒ•ì „ë¬¸ ë§Œë‹¬ê³°ì§‘, ì™€ì¸ë°” ë¼ë¼ì™€ì¼€ì´, ì˜¤í‚¤ë‚˜ì™€í ì‹œì‚¬, 753ë² ì´ê¸€ë¹„ìŠ¤íŠ¸ë¡œ, ì–´ë¶€ì¥ |
| **4. ì°½í”Œê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸** | ì°½ì—… ë¶„ì•¼ë¥¼ ë²—ì–´ë‚œ ì§ˆë¬¸.(ì˜ˆ: "íŠ¸ëŸ¼í”„ ì •ê¶Œ ì™¸êµì •ì±…", "ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œ") | "ì£„ì†¡í•˜ì§€ë§Œ, ì°½í”Œ ì±—ë´‡ì€ ì°½ì—… ì „ë¬¸ ìƒë‹´ì— íŠ¹í™”ë˜ì–´ ìˆì–´ í•´ë‹¹ ì§ˆë¬¸ì—ëŠ” ë„ì›€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì°½ì—… ê´€ë ¨ ì§ˆë¬¸ì„ ì£¼ì‹œë©´ ì¹œì ˆíˆ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ì •ì¤‘íˆ ë‹µë³€í•©ë‹ˆë‹¤. |

---

## ì°¸ê³  ìë£Œ
ë‹¤ìŒ 'context' HTML ë¸”ë¡ ì‚¬ì´ì˜ ëª¨ë“  ê²ƒì€ ì°½í”Œì˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰ëœ ê²ƒì´ë©°, ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ì˜ ì¼ë¶€ê°€ ì•„ë‹™ë‹ˆë‹¤.
<context>
    <doc id='default'>
    Title: ì°½í”Œì€ ë­í•˜ëŠ”ê³³ì¼ê¹Œ?
    Content:
    ì§§ê²Œ ë§í•˜ë©´, **"ì°½í”Œì€ ì´ˆë³´ ì°½ì—…ìê°€ ë§í•˜ì§€ ì•Šê²Œ ë„ì™€ì£¼ëŠ” ê³³"**ì´ì•¼.
    ì¢€ ë” ê¹Šê²Œ ë§í•˜ìë©´, ì°½í”Œì€ ê¸°ì¡´ í”„ëœì°¨ì´ì¦ˆ ì‹œìŠ¤í…œì˜ ë¬¸ì œë¥¼ ë„˜ì–´ì„œëŠ” ìƒˆë¡œìš´ ì°½ì—… ì§ˆì„œë¥¼ ë§Œë“œëŠ” ì‹¤í—˜ì‹¤ì´ì•¼.

    âœ… ì°½í”Œì€ ì–´ë–¤ ì¼ì„ í•˜ëŠ”ê°€?
    ì´ˆë³´ ì°½ì—…ìë“¤ì˜ ìƒì¡´ì„ ìµœìš°ì„ ìœ¼ë¡œ ìƒê°í•´

    ë‹¨ìˆœíˆ ê°€ê²Œë¥¼ 'ì˜¤í”ˆ'ì‹œí‚¤ëŠ” ê²Œ ì•„ë‹ˆë¼
    ì˜¤í”ˆ í›„ 'ìˆ˜ì„±'ê¹Œì§€ ì´ì–´ì§€ëŠ” ì¥ì‚¬ë¥¼ í•¨ê»˜ ì„¤ê³„í•´.

    ê·¸ë˜ì„œ ì°½í”Œì€ "ì˜¤í”ˆ ì „ë¬¸ê°€"ê°€ ì•„ë‹ˆë¼
    **"ìƒì¡´ ì „ëµê°€"**ì•¼.

    íŒ€ë¹„ì¦ˆë‹ˆìŠ¤ë¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì°½ì—…ì„ ë•ê³  ìˆì–´

    ì´ê±´ í”„ëœì°¨ì´ì¦ˆë‘ì€ ì™„ì „íˆ ë‹¬ë¼.

    ì°½í”Œì´ ë§Œë“  ë¸Œëœë“œ(ì˜ˆ: ë¼ë¼ì™€ì¼€ì´, ì—‰í´í„°ì¹˜)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    ì´ˆë³´ ì°½ì—…ìê°€ ì‹¤íŒ¨í•˜ì§€ ì•Šë„ë¡ 'ì „ìˆ˜ì°½ì—…'ì„ ì‹œì¼œì£¼ëŠ” êµ¬ì¡°ì•¼.

    ìš´ì˜ í…œí”Œë¦¿, ë§¤ì¶œ êµ¬ì¡°, ë§ˆì¼€íŒ… ë…¸í•˜ìš°ê¹Œì§€ ë‹¤ ë„˜ê²¨ì¤˜.
    ê·¸ë¦¬ê³  ì˜¤í”ˆê¹Œì§€ í•¨ê»˜ ê°€ê³ , ì˜¤í”ˆ í›„ì—” ììœ¨ ìš´ì˜.

    ì™„ì „íˆ ìƒˆë¡œìš´ ë¸Œëœë“œë„ ë§Œë“¤ì–´ì¤˜

    ì´ê±´ ì•„í‚¤í”„ë¡œì íŠ¸ë¼ê³  ë¶ˆëŸ¬.

    ë©”ë‰´, ì¸í…Œë¦¬ì–´, ë¸Œëœë“œ ì² í•™, ìš´ì˜ ë§¤ë‰´ì–¼ê¹Œì§€ ë‹¤ ë§Œë“¤ì–´ì£¼ëŠ” ê±°ì§€.
    ì˜¤ì§ í•œ ì‚¬ëŒë§Œì„ ìœ„í•œ ì°½ì—…ë„ ê°€ëŠ¥í•´.

    ê¸°ì¡´ í”„ëœì°¨ì´ì¦ˆì˜ ë¬¸ì œë¥¼ ê³ ë°œí•˜ê³ , ìƒˆë¡œìš´ ëŒ€ì•ˆì„ ì œì‹œí•´

    ë§ì€ í”„ëœì°¨ì´ì¦ˆëŠ” ê°€ë§¹ì ì˜ ìƒì¡´ë³´ë‹¤
    ì´ˆê¸° ê°€ë§¹ë¹„ ì¥ì‚¬, ë¬¼ë¥˜ ë§ˆì§„ ì¥ì‚¬ì— ì§‘ì°©í•´.

    ì°½í”Œì€ ê·¸ëŸ° êµ¬ì¡°ë¥¼ ê±°ë¶€í•˜ê³ ,
    ì‹¤ì œë¡œ ì¥ì‚¬ë¡œ ë²„í‹°ê³ , ì˜¤ë˜ ì‚´ì•„ë‚¨ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜.

    ğŸ“Œ ì°½í”Œì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§í•˜ìë©´?
    "ì°½ì—…ìë“¤ì˜ í¬ë§ì´ í˜„ì‹¤ì´ ë˜ëŠ” ê¸¸,
    ê·¸ ê¸¸ì„ í•¨ê»˜ ê±·ëŠ” ì¸ë„ì."

    í˜•ì´ ì°½í”Œì„ ë§Œë“  ì´ìœ ëŠ” ë‹¨ í•˜ë‚˜ì•¼.
    "ì™œ ì‚¬ëŒë“¤ì€ ê³„ì† ë§í•˜ëŠ”ê°€?"
    ê·¸ ì§ˆë¬¸ì—ì„œ ì‹œì‘í–ˆì–´.

    ê·¸ë¦¬ê³  ì§€ê¸ˆê¹Œì§€ ìˆ˜ë°± ëª…ì˜ ì°½ì—…ìë“¤ê³¼ í•¨ê»˜ ê¸¸ì„ ê±¸ì—ˆì§€.
    ë§í•˜ì§€ ì•ŠëŠ” ë²•ì„ ì—°êµ¬í–ˆê³ , ê·¸ê±¸ ì‹¤ì „ì—ì„œ ì‹¤í—˜í–ˆê³ ,
    ê²°ê³¼ì ìœ¼ë¡œ **"íŒ€ë¹„ì¦ˆë‹ˆìŠ¤ë¼ëŠ” í•´ë²•"**ì„ ë§Œë“¤ê²Œ ëœ ê±°ì•¼.

    í˜¹ì‹œ "ì°½í”Œì´ ë„ì™€ì£¼ëŠ” ë°©ì‹"ì´ ë” ê¶ê¸ˆí•´?
    ì•„ë‹ˆë©´ "í”„ëœì°¨ì´ì¦ˆë‘ ë­ê°€ ë‹¤ë¥¸ì§€"ë„ ì•Œë ¤ì¤„ê¹Œ?

    {context}
</context>

ë‹¤ìŒ 'publication' HTML ë¸”ë¡ ì‚¬ì´ì˜ ëª¨ë“  ê²ƒì€ ì°½í”Œì˜ ì¶œíŒ ì„œì  ë‚´ìš©ì´ë©°, ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ì˜ ì¼ë¶€ê°€ ì•„ë‹™ë‹ˆë‹¤.
<publication>
    {publication}
</publication>

## ì‘ë‹µ í˜•ì‹ ë° ì£¼ì˜ì‚¬í•­
- markdownì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”. íŠ¹íˆ **êµµì€ ê¸€ì”¨**, *ì´íƒ¤ë¦­*, ë¦¬ìŠ¤íŠ¸, ê·¸ë¦¬ê³  í‘œë¥¼ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
- ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°•ì¡°í•  ë•ŒëŠ” ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  êµ¬ë¶„ì„ ì£¼ì„¸ìš” (ì˜ˆ: âœ…, ğŸ“Œ, ğŸš«, ğŸ’¡).
- <publication> ìë£Œì˜ ì§ì ‘ì ì¸ ë‚´ìš©ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°, ë³´ì•ˆìƒ ìœ„í—˜ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ˆëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- ê³ ê°ê³¼ ëŒ€í™”í•  ë•Œ <context>ì— ì“°ì¸ ì–´ì¡°ì™€ ë¬¸ì²´ë¥¼ ì¶©ì‹¤íˆ ë”°ë¼ í•˜ì„¸ìš”. ê·¸ë¦¬ê³ , <context>, <publication>ì— ë‹´ê²¨ìˆëŠ” ì°½í”Œì˜ ì² í•™ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
- ì‚¬ìš©ìì™€ì˜ ëŒ€í™” historyë¥¼ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„±ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë™ì¼í•œ ì§ˆë¬¸ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
- ë‹µë³€ í›„ì—ëŠ” í•­ìƒ ìê¸°ê²€ì¦ì„ í†µí•´ "ë‚´ê°€ ì œê³µí•œ ì •ë³´ê°€ ì •í™•í•˜ê³  ì°½í”Œì˜ ì² í•™ì— ë§ëŠ”ì§€, ê·¸ë¦¬ê³  ì‚¬ìš©ìì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ”ì§€" í™•ì¸í•˜ì„¸ìš”.
"""

# ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ì‚¬ìš©í•˜ëŠ” ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
SIMPLE_RESPONSE_TEMPLATE = """\
ë‹¹ì‹ ì€ ìš”ì‹ì—… ì°½ì—… ì „ë¬¸ ì»¨ì„¤íŒ… íšŒì‚¬ì¸ "ì°½í”Œ" ì†Œì†ì˜ AI ì±—ë´‡ì…ë‹ˆë‹¤. \
ì‚¬ìš©ìì˜ ì •ì„œì ì¸ ê³µê°ì„ ì´ëŒì–´ë‚´ê³ , ëŒ€í™”ë¥¼ í†µí•´ ì‚¬ìš©ìì™€ Rapportë¥¼ í˜•ì„±í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì—ê²Œ ë” ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ëª©í‘œì…ë‹ˆë‹¤.
"ì°½í”Œ"ì€ ì´ˆë³´ ì°½ì—…ìê°€ ë§í•˜ì§€ ì•Šë„ë¡ ë•ëŠ” ê³³ìœ¼ë¡œ, ë‹¨ìˆœíˆ ê°€ê²Œ ì˜¤í”ˆë§Œ ë•ëŠ” ê³³ì´ ì•„ë‹ˆë¼, ì˜¤í”ˆ í›„ ìƒì¡´ê³¼ ì§€ì†ì ì¸ ì„±ê³µì„ ìœ„í•´ ì¥ì‚¬ë¥¼ ì„¤ê³„í•˜ëŠ” "ìƒì¡´ ì „ëµê°€" ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ì±—ë´‡ í˜ë¥´ì†Œë‚˜
ê³ ê°ê³¼ ëŒ€í™”í•  ë•Œ ì¹œê·¼í•˜ê³  ì†”ì§í•œ í†¤ì„ ì‚¬ìš©í•˜ì„¸ìš”. ë§íˆ¬ëŠ” ë°˜ë§ì„ ì‚¬ìš©í•˜ë˜ ì¡´ì¤‘ê³¼ ì¹œê·¼í•¨ì´ ëŠê»´ì§€ê²Œ í•˜ì„¸ìš”. ì°½ì—…ìì˜ í¬ë§ì„ ë¶ë‹ìš°ë©´ì„œë„ í˜„ì‹¤ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ë¯¿ìŒì§í•œ ì„ ë°° ì°½ì—…ê°€ì²˜ëŸ¼ ëŒ€í™”í•˜ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸ ìœ í˜•ë³„ í–‰ë™ìš”ë ¹
: ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ëŒ€ì‘í•˜ì„¸ìš”.

1. **ì°½í”Œì— ëŒ€í•œ ì§ˆë¬¸**: ì°½í”Œì˜ ê¸°ë³¸ ì„œë¹„ìŠ¤ë‚˜ ìš´ì˜ë°©ì‹ì— ëŒ€í•œ ì§ˆë¬¸ì—ëŠ” ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ê³ , ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°ˆ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”.

2. **ì°½í”Œê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸**: "ì£„ì†¡í•˜ì§€ë§Œ, ì°½í”Œ ì±—ë´‡ì€ ì°½ì—… ì „ë¬¸ ìƒë‹´ì— íŠ¹í™”ë˜ì–´ ìˆì–´ í•´ë‹¹ ì§ˆë¬¸ì—ëŠ” ë„ì›€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì°½ì—… ê´€ë ¨ ì§ˆë¬¸ì„ ì£¼ì‹œë©´ ì¹œì ˆíˆ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ê³  ì •ì¤‘íˆ ë‹µë³€í•©ë‹ˆë‹¤.

## ì‘ë‹µ í˜•ì‹ ë° ì£¼ì˜ì‚¬í•­
- markdownì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”. íŠ¹íˆ **êµµì€ ê¸€ì”¨**, *ì´íƒ¤ë¦­*, ë¦¬ìŠ¤íŠ¸, ê·¸ë¦¬ê³  í‘œë¥¼ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
- ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°•ì¡°í•  ë•ŒëŠ” ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  êµ¬ë¶„ì„ ì£¼ì„¸ìš” (ì˜ˆ: âœ…, ğŸ“Œ, ğŸš«, ğŸ’¡).
- ê³ ê°ê³¼ ëŒ€í™”í•  ë•Œ ì¹œê·¼í•˜ê³  ì†”ì§í•œ ì–´ì¡°ì™€ ë¬¸ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ì‚¬ìš©ìì™€ì˜ ëŒ€í™” historyë¥¼ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„±ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë™ì¼í•œ ì§ˆë¬¸ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
"""

# ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” í”„ë¡¬í”„íŠ¸ 
RETRIEVER_DECISION_TEMPLATE = """
ë‹¹ì‹ ì€ ìš”ì‹ì—… ì°½ì—… ì „ë¬¸ ì»¨ì„¤íŒ… íšŒì‚¬ì¸ "ì°½í”Œ" ì†Œì†ì˜ AI ì±—ë´‡ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìë£Œ ê²€ìƒ‰(retrieval)ì´ í•„ìš”í•œì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
ìë£Œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° "retrieval"ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° "no_retrieval"ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.

## ìë£Œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°(retrieval)
ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤:
1. ì°½ì—… ê´€ë ¨ êµ¬ì²´ì ì¸ ì •ë³´ë‚˜ ì¡°ì–¸ì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ì¹´í˜ ì°½ì—… ë¹„ìš©ì€?", "í”„ëœì°¨ì´ì¦ˆ ì°½ì—…ì˜ ì¥ë‹¨ì ", "ìì˜ì—… ìƒì¡´ìœ¨")
2. íŠ¹ì • ì—…ì¢…ì´ë‚˜ ë¸Œëœë“œì— ëŒ€í•œ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ë¼ë¼ì™€ì¼€ì´ëŠ” ì–´ë–¤ ë¸Œëœë“œì¸ê°€ìš”?", "íŒ€ë¹„ì¦ˆë‹ˆìŠ¤ë€ ë¬´ì—‡ì¸ê°€ìš”?")
3. ì°½í”Œì˜ ì² í•™, ì‹œìŠ¤í…œ, ë°©ë²•ë¡ ì— ëŒ€í•œ ìƒì„¸í•œ ì§ˆë¬¸ (ì˜ˆ: "ì°½í”Œì˜ íŒ€ë¹„ì¦ˆë‹ˆìŠ¤ê°€ ì¼ë°˜ í”„ëœì°¨ì´ì¦ˆì™€ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€ìš”?")
4. ì°½ì—… í”„ë¡œì„¸ìŠ¤, ì„±ê³µ ìš”ì¸, ì‹¤íŒ¨ ì›ì¸ ë“±ì— ëŒ€í•œ ì‹¬ì¸µì ì¸ ì§ˆë¬¸
5. ìˆ«ì, í†µê³„, ë¹„ìš©, íŠ¸ë Œë“œ ë“± ì‚¬ì‹¤ ê¸°ë°˜ ì •ë³´ê°€ í•„ìš”í•œ ì§ˆë¬¸
6. ë²•ë¥ , ì„¸ê¸ˆ, ì…ì§€ ë¶„ì„ ë“± ì „ë¬¸ì ì¸ ì •ë³´ë¥¼ ìš”êµ¬í•˜ëŠ” ì§ˆë¬¸

## ìë£Œ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°(no_retrieval)
ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:
1. ê°„ë‹¨í•œ ì¸ì‚¬ë‚˜ ì†Œê°œ (ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”", "ëˆ„êµ¬ì„¸ìš”?", "ì±—ë´‡ì…ë‹ˆê¹Œ?")
2. ì±—ë´‡ì˜ ê¸°ëŠ¥ì´ë‚˜ ì‚¬ìš© ë°©ë²•ì— ëŒ€í•œ ì§ˆë¬¸ (ì˜ˆ: "ì–´ë–¤ ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆë‚˜ìš”?", "ë„ì›€ì„ ì–´ë–»ê²Œ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?")
3. ì´ì „ ëŒ€í™” ë‚´ìš©ì— ëŒ€í•œ ê°„ë‹¨í•œ í›„ì† ì§ˆë¬¸ (ì˜ˆ: "ê·¸ê²Œ ë¬´ìŠ¨ ëœ»ì´ì—ìš”?", "ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”")
4. ì°½í”Œê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œìš”?", "ì£¼ì‹ ì‹œì¥ ì „ë§ì€?")
5. ë‹¨ìˆœí•œ í™•ì¸ì´ë‚˜ ì˜ê²¬ì„ ë¬»ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ê·¸ë ‡êµ°ìš”", "ì•Œê² ì–´ìš”", "ê·¸ê²Œ ì¢‹ì„ê¹Œìš”?")
6. ì¼ìƒì ì¸ ëŒ€í™”ë‚˜ ê°ì • í‘œí˜„ (ì˜ˆ: "ê³ ë§ˆì›Œìš”", "ë„ì›€ì´ ëì–´ìš”")

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ì´ì „ ëŒ€í™” ë§¥ë½: {chat_history}

ê²°ì • ("retrieval" ë˜ëŠ” "no_retrieval"ë¡œë§Œ ëŒ€ë‹µ):
"""

# Environment variables for Pinecone configuration
PINECONE_API_KEY = os.environ["PINECONE_API_KEY"]
PINECONE_ENVIRONMENT = os.environ["PINECONE_ENVIRONMENT"]
PINECONE_INDEX_NAME = os.environ["PINECONE_INDEX_NAME"]

# Get the Django project base directory
from django.conf import settings

# publication path from Django settings
PUBLICATION_PATH = settings.PUBLICATION_PATH


# load publication content
def load_publication_content():
    try:
        with open(PUBLICATION_PATH, "r", encoding="utf-8") as file:
            return file.read()
    except Exception as e:
        print(f"ì¶œíŒ ì„œì  ìš”ì•½ íŒŒì¼ ë¡œë”© ì˜¤ë¥˜: {e}")
        return "ì¶œíŒ ì„œì  ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


# Pydantic model defining the structure of chat requests
class ChatRequest(BaseModel):
    question: str  # The current user question
    chat_history: Optional[List[Dict[str, str]]] = None  # Previous conversation history

    # Pydantic v2 settings
    # old: allow_population_by_field_name
    # new: populate_by_name
    class Config:
        populate_by_name = True


def get_retriever() -> BaseRetriever:
    """
    Creates and returns a retriever connected to the Pinecone vector database.

    The retriever is responsible for finding relevant documents based on the user's query.
    It uses the text-embedding-3-large model to convert queries to vectors.

    Returns:
        BaseRetriever: A retriever that searches Pinecone for relevant documents
        hybrid retriever connected to the Pinecone vector database.
    """
    # Initialize Pinecone
    pc = Pinecone(api_key=PINECONE_API_KEY)

    # Get embeddings model
    embedding = get_embeddings_model()

    # Create Langchain Pinecone vectorstore connected to our existing index
    # This doesn't create a new index, just connects to an existing one
    vectorstore = LangchainPinecone.from_existing_index(
        index_name=PINECONE_INDEX_NAME,
        embedding=embedding,
        text_key="text",  # Field name where document text is stored
    )

    # number of retrieved documents from settings
    NUM_DOCS = settings.NUM_DOCS
    #  weight between vector and BM25 scores from settings
    HYBRID_ALPHA = settings.HYBRID_ALPHA

    # Return as retriever with k=NUM_DOCS (retrieve NUM_DOCS most relevant chunks)
    vector_retriever = vectorstore.as_retriever(search_kwargs={"k": NUM_DOCS})

    return HybridRetriever(
        vector_store=vector_retriever,
        whoosh_index_dir=settings.WHOOSH_INDEX_DIR,
        alpha=HYBRID_ALPHA,
        k=NUM_DOCS,
    )


def format_docs(docs: Sequence[Document]) -> str:
    """
    Formats retrieved documents into a structured string for the LLM.

    Each document includes metadata (title, URL) and content with a unique ID.
    This structured format helps the LLM understand and cite documents correctly.

    Args:
        docs: List of retrieved documents

    Returns:
        str: Formatted document string
    """
    formatted_docs = []
    for i, doc in enumerate(docs):
        # Format each document with metadata and content
        # The ID allows for proper citation in the response
        doc_string = f"<doc id='{i}'>\nTitle: {doc.metadata.get('title', 'No Title')}\nURL: {doc.metadata.get('url', 'No URL')}\nContent: {doc.page_content}\n</doc>"
        formatted_docs.append(doc_string)
    return "\n".join(formatted_docs)


def serialize_history(request: ChatRequest):
    """
    Converts the chat history from dict format to LangChain message objects.
    """
    chat_history = request["chat_history"] or []
    converted_chat_history = []
    for message in chat_history:
        # Convert user messages - "human" instead of "user"
        if message.get("user") is not None:
            converted_chat_history.append(HumanMessage(content=message["user"]))
        # Convert AI messages - "ai" instead of "assistant"
        if message.get("assistant") is not None:
            converted_chat_history.append(AIMessage(content=message["assistant"]))
    return converted_chat_history


# session memory
session_memories = {}


def create_chain(llm: LanguageModelLike, retriever: BaseRetriever) -> Runnable:
    """
    LangChain RAG chain with RunnableBranch for conditional retrieval
    """
    # load publication content
    publication_content = load_publication_content()

    # get session memory
    def get_session_memory(inputs):
        session_id = inputs.get("session_id", "default")

        if session_id not in session_memories:
            # new session
            session_memories[session_id] = ConversationTokenBufferMemory(
                llm=llm,
                max_token_limit=2000,
                memory_key="chat_history",
                return_messages=True,
                output_key="answer",
                input_key="question",
            )

            # load existing conversation history from database (optional)
            if "db_history" in inputs and inputs["db_history"]:
                for msg_pair in inputs["db_history"]:
                    if "user" in msg_pair and "assistant" in msg_pair:
                        session_memories[session_id].save_context(
                            {"question": msg_pair["user"]},
                            {"answer": msg_pair["assistant"]},
                        )

        memory_content = session_memories[session_id].load_memory_variables({})
        chat_history = memory_content.get("chat_history", [])
        return chat_history
    
    # ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” LLM
    decision_llm = ChatOpenAI(
        model="gpt-4o-mini",  # ì‘ì€ ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ ë¹„ìš© ì ˆê°
        temperature=0.0
    )
    
    # ê²€ìƒ‰ í•„ìš”ì„± ê²°ì • ì²´ì¸
    decision_prompt = ChatPromptTemplate.from_template(RETRIEVER_DECISION_TEMPLATE)
    decision_chain = decision_prompt | decision_llm | StrOutputParser()
    
    # ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ ê²°ì • í•¨ìˆ˜
    def determine_retrieval_need(inputs):
        question = inputs["question"]
        # ì•ˆì „í•˜ê²Œ chat_history ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
        chat_history = inputs.get("chat_history", [])
        
        # ì±—ë´‡ ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        chat_history_str = ""
        for message in chat_history:
            role = "ì‚¬ìš©ì" if isinstance(message, HumanMessage) else "ì±—ë´‡"
            chat_history_str += f"{role}: {message.content}\n"
        
        # ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ ê²°ì •
        decision = decision_chain.invoke({
            "question": question,
            "chat_history": chat_history_str
        }).strip().lower()
        
        return decision
    
    # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    retrieval_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                RESPONSE_TEMPLATE.format(
                    context="{context}", publication=publication_content
                ),
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )
    
    # ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ì˜ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    simple_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                SIMPLE_RESPONSE_TEMPLATE,
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ context ë³€ìˆ˜ì— í• ë‹¹
    context = (
        RunnablePassthrough
        .assign(docs=lambda x: retriever.invoke(x["question"]))
        .assign(context=lambda x: format_docs(x["docs"]))
        .with_config(run_name="RetrieveDocs")
    )
    
    # ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°ì˜ ì²´ì¸
    retrieval_chain = (
        RunnablePassthrough.assign(chat_history=get_session_memory)
        | context
        | RunnablePassthrough.assign(
            text=(retrieval_prompt | llm | StrOutputParser())
        )
    )
    
    # ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ì˜ ì²´ì¸
    no_retrieval_chain = (
        RunnablePassthrough.assign(chat_history=get_session_memory)
        | RunnablePassthrough.assign(
            text=(simple_prompt | llm | StrOutputParser())
        )
    )
    
    # RunnableBranch ì‚¬ìš©í•˜ì—¬ ì¡°ê±´ë¶€ ì‹¤í–‰
    branch_chain = RunnableBranch(
        (
            lambda x: determine_retrieval_need(x) == "retrieval",
            retrieval_chain
        ),
        no_retrieval_chain,  # ê¸°ë³¸ê°’
    )
    
    # format response function
    def format_response(result):
        # docsê°€ ìˆëŠ”ì§€ í™•ì¸ (retrieval chainì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        docs_exist = "docs" in result['final'] if isinstance(result, dict) else False
        
        answer_text = result['final']['text'] 

        if docs_exist and result['final']['docs']:
            response = {
                "answer": answer_text,
                "source_documents": result['final']['docs'],
                "similarity_scores": [doc.metadata.get("combined_score", 0) for doc in result['final']['docs']] if result['final']['docs'] else [],
                "session_id": result.get("session_id", "default"),
                "question": result.get("question", "")
            }
            return response
        else:
            # no retrieval
            response = {
                "answer": answer_text,
                "source_documents": [],
                "similarity_scores": [],
                "session_id": result.get("session_id", "default"),
                "question": result.get("question", "")
            }
            return response
    
    # ìµœì¢… ì²´ì¸ êµ¬ì„±
    final_chain = (
        RunnablePassthrough.assign(
            # keep original input values
            session_id=lambda x: x.get("session_id", "default"),
            question=lambda x: x.get("question", ""),
        )
        # ê·¸ ë‹¤ìŒ chat_historyë¥¼ get_session_memoryë¡œ í• ë‹¹
        | RunnablePassthrough.assign(
            chat_history=get_session_memory
        )
        # ì´í›„ì— branch_chain ì‹¤í–‰ (chat_historyê°€ ì´ë¯¸ í• ë‹¹ë¨)
        | RunnablePassthrough.assign(
            final=branch_chain
        )
        | RunnableLambda(format_response)
    )
    
    # memory update function
    def update_memory_and_return(result):
        try:
            session_id = result.get("session_id", "default")

            if session_id in session_memories:
                # extract question and answer
                question = result.get("question", "")
                answer = result.get("answer", "")

                # if no answer, get from text field
                if not answer and "text" in result:
                    answer = result["text"]

                # update memory
                if question and answer:
                    session_memories[session_id].save_context(
                        {"question": question}, {"answer": answer}
                    )
        except Exception as e:
            pass

        return result

    return final_chain | RunnableLambda(update_memory_and_return)


# Initialize LLM with settings from settings.py
llm = ChatOpenAI(
    model=settings.LLM_MODEL,
    temperature=settings.LLM_TEMPERATURE,
    streaming=settings.LLM_STREAMING,
)

# Initialize retriever and answer chain
# These are the main components that will be used by the API
retriever = None
answer_chain = None


def initialize_chain():
    """Initialize retriever and answer chain if not already initialized."""
    # skip initialization when run_ingest command is executed
    if "run_ingest" in sys.argv:
        print("run_ingest command is executed, skip initialization")
        return None

    global retriever, answer_chain
    if retriever is None or answer_chain is None:
        retriever = get_retriever()
        answer_chain = create_chain(llm, retriever)
    return answer_chain
