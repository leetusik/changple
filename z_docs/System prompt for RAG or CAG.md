### Key Points
- Research suggests the system prompt for CAG (Cache-Augmented Generation) is typically a standard instruction like "You are an AI assistant with expert knowledge."
- It seems likely that there is no unique system prompt for CAG, as it depends on implementation, with knowledge and queries combined in the user prompt.
- The evidence leans toward the system prompt being optional, with the focus on preloading knowledge into the model's context for efficient query handling.

### System Prompt Overview
The system prompt in CAG is generally a standard instruction given to the large language model (LLM) to set its role, such as "You are an AI assistant with expert knowledge." This helps guide the model in using preloaded knowledge to answer user queries efficiently. However, the exact system prompt can vary based on how CAG is implemented, as it’s more about the method of handling knowledge and queries rather than a fixed prompt.

### Knowledge and Query Integration
In CAG, the preloaded knowledge (often called the context) and the user's query are typically combined in the user prompt, formatted in ways like "Context: [knowledge]\n\nQuery: [query]\nAnswer:" This setup allows the model to leverage cached information for faster responses, bypassing real-time retrieval seen in Retrieval-Augmented Generation (RAG).

### Implementation Variability
Given the flexibility in CAG implementations, there isn’t a universal system prompt. Some examples use a generic system message, while others might not use one at all, focusing instead on the structure of the user prompt. This variability reflects the adaptability of CAG to different use cases, such as static knowledge bases or latency-sensitive applications.

---

### Survey Note: Detailed Analysis of CAG System Prompt

This note provides a comprehensive exploration of the system prompt for Cache-Augmented Generation (CAG), a method designed to enhance large language models (LLMs) by preloading knowledge into the model's context and caching its runtime parameters, thus eliminating the need for real-time retrieval as seen in Retrieval-Augmented Generation (RAG). The analysis is based on recent research and practical implementations, aiming to clarify the concept for both technical and non-technical audiences.

#### Background on CAG
CAG has emerged as an alternative to RAG, addressing challenges such as retrieval latency, errors in document selection, and increased system complexity. By leveraging the extended context windows of modern LLMs, CAG preloads all relevant resources into the model's context and caches its key-value (KV) pairs. This approach enables faster inference by allowing the model to generate responses directly from the cached state, particularly suitable for static knowledge bases, latency-sensitive applications, and environments with low infrastructure overhead.

Research, such as the paper "Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks" ([Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks](https://arxiv.org/html/2412.15605v1)), highlights that CAG can eliminate retrieval latency and minimize errors while maintaining context relevance, making it ideal for knowledge-intensive tasks where the knowledge corpus fits within the model's context window.

#### Understanding System Prompt in LLMs
In the context of LLMs, a system prompt typically refers to an initial message with the role "system," setting instructions or context for the assistant. For example, in OpenAI's Chat API, a common system prompt might be "You are a helpful assistant." This message guides the model's behavior before user interactions begin. In CAG, the system prompt's role is to integrate with the preloaded knowledge, ensuring the model understands how to use this cached information to answer queries.

#### Analysis of System Prompt in CAG
The investigation into the system prompt for CAG reveals that there is no universally defined prompt specific to this method, as it depends on the implementation. However, several insights can be drawn from available resources:

- **Standard System Messages in Examples**: Tutorials and implementations, such as those found on Medium, often use a generic system prompt like "You are an AI assistant with expert knowledge." For instance, in the article "Cache-Augmented Generation (CAG) in LLMs: A Step-by-Step Tutorial" ([Cache-Augmented Generation (CAG) in LLMs: A Step-by-Step Tutorial](https://medium.com/@ronantech/cache-augmented-generation-cag-in-llms-a-step-by-step-tutorial-6ac35d415eec)), the system prompt is set as "You are an AI assistant with expert knowledge," followed by a user prompt that includes both the preloaded knowledge and the query, formatted as "Context:\n{context}\n\nQuery: {query}\nAnswer:". This suggests a common practice of using a standard instruction to set the model's role, with the knowledge integrated into the user message.

- **Prompt Construction in Practice**: The original paper on CAG, as seen in the GitHub repository [Cache-Augmented Generation: A Simple, Efficient Alternative to RAG](https://github.com/hhhuang/CAG), describes the inference process where the model is given a combined prompt \(\mathcal{P} = \text{Concat}(\mathcal{D}, \mathcal{Q})\), with \(\mathcal{D}\) being the preloaded knowledge and \(\mathcal{Q}\) the user's query. However, it does not explicitly mention a system message, indicating that the focus is on the concatenation rather than additional instructions. This suggests that in some implementations, there might not be a separate system prompt, and the entire prompt is treated as a user message.

- **Variability Across Implementations**: Different sources, such as the Medium article "Cache Augmented Generation (CAG) from Scratch" ([Cache Augmented Generation (CAG) from Scratch](https://medium.com/@sabaybiometzger/cache-augmented-generation-cag-from-scratch-441adf71c6a3)), show similar patterns where the system prompt remains generic, and the knowledge is preloaded into the user prompt. This variability is evident in the usage examples provided in the GitHub repository, where scripts like kvcache.py and rag.py handle datasets like SQuAD and HotpotQA, but do not specify a default system prompt, focusing instead on parameters like dataset, model name, and output files.

#### Detailed Examination of Prompt Formats
To further clarify, let's examine how prompts are constructed in CAG:

| Script       | Example Command                                                                                     | Parameters                                                                                     |
|--------------|----------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| kvcache.py   | python ./kvcache.py --kvcache file --dataset "squad-train" --similarity bertscore --maxKnowledge 5 --maxParagraph 100 --maxQuestion 1000 --modelname "meta-llama/Llama-3.1-8B-Instruct" --randomSeed 0 --output "./result_kvcache.txt" | --kvcache: "file", --dataset: "hotpotqa-train" or "squad-train", --similarity: "bertscore", --modelname: "meta-llama/Llama-3.1-8B-Instruct", --maxKnowledge: "", int, --maxParagraph: 100, --maxQuestion: int, --randomSeed: "", int, --output: "", str, --usePrompt: optional |
| rag.py       | python ./rag.py --index "bm25" --dataset "hotpotqa-train" --similarity bertscore --maxKnowledge 80 --maxParagraph 100 --maxQuestion 80 --topk 3 --modelname "meta-llama/Llama-3.1-8B-Instruct" --randomSeed 0 --output "./rag_results.txt" | --index: "openai" or "bm25", --dataset: "hotpotqa-train" or "squad-train", --similarity: "bertscore", --maxKnowledge: "", int, --maxParagraph: 100, --maxQuestion: int, --topk: int, --modelname: "meta-llama/Llama-3.1-8B-Instruct", --randomSeed: "", int, --output: "", str |

Additional notes from the repository indicate that for --maxKnowledge with "squad-train," values like k=3 (21,000 tokens) are used, and for "hotpotqa-train," k=1 (1,400 tokens) to k=80 (106,000 tokens) are specified, suggesting the scale of knowledge preloaded. However, the prompt construction itself is not detailed, implying flexibility in how knowledge and queries are formatted.

#### Theoretical and Practical Implications
Theoretically, CAG's efficiency stems from preloading and caching, which reduces latency by avoiding real-time retrieval. Practically, this means the system prompt, if used, should ensure the model understands its role in leveraging cached knowledge. For example, a system prompt like "You have been provided with a cache of knowledge. Use this knowledge to answer the user's question accurately." could be more specific to CAG, though such examples are not universally adopted in the literature reviewed.

Discussions on platforms like Reddit, such as in r/LangChain ([Cache Augmented Generation](https://www.reddit.com/r/LangChain/comments/1jh759u/cache_augmented_generation/)), suggest that for CAG, static information should be at the beginning of the prompt to increase cache hit likelihood, reinforcing the idea that the system prompt is less critical than the structure of the user prompt. This aligns with the observation that modern LLMs can handle large contexts, reducing the need for complex retrieval systems.

#### Conclusion
In conclusion, the system prompt for CAG is not strictly defined and varies by implementation. Research and examples suggest it is often a standard instruction like "You are an AI assistant with expert knowledge," with the preloaded knowledge and user's query combined in the user prompt, typically formatted as "Context: [knowledge]\n\nQuery: [query]\nAnswer:". This flexibility reflects CAG's adaptability to different use cases, such as static knowledge bases or latency-sensitive applications, as discussed in articles like "Beyond RAG: How cache-augmented generation reduces latency, complexity for smaller workloads" ([Beyond RAG: How cache-augmented generation reduces latency, complexity for smaller workloads](https://venturebeat.com/ai/beyond-rag-how-cache-augmented-generation-reduces-latency-complexity-for-smaller-workloads/)). Given the lack of a universal standard, practitioners are encouraged to tailor the system prompt based on their specific needs, ensuring alignment with the model's capabilities and the application's requirements.

#### Key Citations
- [Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks](https://arxiv.org/html/2412.15605v1)
- [Cache-Augmented Generation (CAG) in LLMs: A Step-by-Step Tutorial](https://medium.com/@ronantech/cache-augmented-generation-cag-in-llms-a-step-by-step-tutorial-6ac35d415eec)
- [Cache Augmented Generation (CAG) from Scratch](https://medium.com/@sabaybiometzger/cache-augmented-generation-cag-from-scratch-441adf71c6a3)
- [Cache-Augmented Generation: A Simple, Efficient Alternative to RAG](https://github.com/hhhuang/CAG)
- [Beyond RAG: How cache-augmented generation reduces latency, complexity for smaller workloads](https://venturebeat.com/ai/beyond-rag-how-cache-augmented-generation-reduces-latency-complexity-for-smaller-workloads/)
- [Cache Augmented Generation](https://www.reddit.com/r/LangChain/comments/1jh759u/cache_augmented_generation/)


---

### Gemini deep research Results
A common template structure is: "Use the following context to answer the question below. Context:. Question: [User Query]".3 Similarly, another example suggests the format: "DOCUMENT: (document text) QUESTION: (users question) INSTRUCTIONS: Answer the users QUESTION using the DOCUMENT text above. Keep your answer ground in the facts of the DOCUMENT.".12 These templates directly guide the LLM to focus on the provided information, ensuring that the generated response is grounded in the retrieved context and directly addresses the user's question. This direct guidance helps improve answer accuracy and relevance by making the connection between the retrieved information and the query explicit.Furthermore, effective system prompts in RAG often include instructions on how to handle scenarios where the retrieved documents are insufficient or not relevant to answer the user's question.12 For example, a prompt might instruct the LLM that if the provided materials are not relevant or complete enough to confidently answer the user's question, the appropriate response is "the materials do not appear to be sufficient to provide a good answer.".12 This type of instruction is important for managing user expectations and preventing the LLM from generating potentially inaccurate or speculative answers when it lacks adequate information in the retrieved context. By guiding the LLM on how to acknowledge its limitations in such cases, the system can provide a more transparent and trustworthy user experience.System prompts also play a vital role in text generation tasks within a RAG framework. These prompts can instruct the LLM to generate specific types of creative content based on the information retrieved from external sources.10 For example, a task-oriented prompt might be: "Write a news article about climate change based on the latest sources," where the "latest sources" would be the documents retrieved by the RAG system.10 This demonstrates how RAG can be leveraged for content creation by grounding the generated text in factual information obtained through retrieval.In addition to guiding the content, system prompts in RAG can also specify the desired style and tone for the generated text, ensuring that it aligns with the context and nature of the retrieved documents, as well as the intended audience.10 RAG prompt engineering emphasizes the importance of tailoring the language style and tone for seamless information delivery.10 For instance, a system prompt might instruct the LLM to adopt a formal and objective tone when summarizing scientific research findings or a more conversational and engaging tone when explaining a concept to a general audience. Examples from research include setting the tone for an AI research assistant to be either "technical and scientific" or "easy to understand".13Frameworks like LlamaIndex provide prompt templates that offer a structured way to incorporate retrieved context into the prompt for various text generation tasks in RAG.14 These templates often include placeholders for the retrieved context and the user's query, allowing for easy customization and ensuring consistency in the prompt structure. For example, a template might look like this: "Context information is below. --------------------- {context_str} --------------------- Given the context information and not prior knowledge, answer the query. Query: {query_str} Answer:".14 By using such templates, developers can streamline the process of designing effective prompts for RAG-based text generation.Designing effective system prompts for RAG requires attention to clarity, conciseness, and the inclusion of relevant context. Best practices include emphasizing positive language and providing clear instructions using "DOs" rather than "DON'Ts".15 It is also crucial to instruct the model to use only the provided context and to cite sources whenever possible.16 Clearly defining the role and task of the LLM within the RAG system helps to focus its behavior.15 For complex tasks, breaking them down into smaller, more manageable steps within the prompt can lead to better results.15 Considering the target audience and tailoring the tone and style of the prompt accordingly is also essential.15 Using specific and clear language while avoiding ambiguity in the instructions further enhances the effectiveness of the system prompt.17 Finally, providing a logical structure to the prompt makes it easier for the LLM to understand and follow the instructions.15System prompts can be strategically used to guide the tone, style, and format of the outputs generated by RAG models.19 Explicitly specifying the desired tone, such as formal, informal, or enthusiastic, in the system prompt sets the overall communication style.9 Providing style guidelines, such as the required reading level or the specific citation format to use, ensures consistency and appropriateness.12 Defining the desired format for the output, whether it be in paragraphs, bullet points, JSON, or code snippets, helps the LLM structure its response in a user-friendly manner.12 Using delimiters within the prompt to clearly separate the context, the question, and the instructions can further improve the model's ability to understand and process the input effectively.12 Lastly, incorporating few-shot examples into the system prompt that demonstrate the desired tone, style, and format can serve as effective visual guides for the LLM to emulate.17